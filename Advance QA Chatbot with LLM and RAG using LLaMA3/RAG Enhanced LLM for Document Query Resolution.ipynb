{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wFas6rxM8vcz"
      },
      "source": [
        "### **Install and Import libarary**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVEqKlBJ8lWh",
        "outputId": "69f30612-71c3-4e98-8d7e-843a5d419bf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.19)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.35 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.40)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.6)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.10.6)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain) (9.0.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.35->langchain) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.35->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.18)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.37 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.40)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.19 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.19)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.38)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.13)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.0.0)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.8.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.11)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy<2,>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (1.26.4)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.18.3)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (0.3.6)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.19->langchain_community) (2.10.6)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.37->langchain_community) (4.12.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (3.10.15)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.37->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.19->langchain_community) (2.27.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain_community) (1.3.1)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.3.0)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (0.6.3)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.10.6)\n",
            "Requirement already satisfied: chroma-hnswlib==0.7.6 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.7.6)\n",
            "Requirement already satisfied: fastapi>=0.95.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.115.10)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
            "Requirement already satisfied: numpy>=1.22.5 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.26.4)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.18.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.12.2)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.20.1)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.30.0)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.0)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.70.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.15.1)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (32.0.1)\n",
            "Requirement already satisfied: tenacity>=8.2.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (9.0.0)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.0.2)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.15)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi>=0.95.2->chromadb) (0.46.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.7)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.8.2)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.27.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
            "Requirement already satisfied: importlib-metadata<=8.5.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.5.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.68.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-proto==1.30.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.30.0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-instrumentation==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: opentelemetry-util-http==0.51b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.51b0)\n",
            "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
            "Requirement already satisfied: asgiref~=3.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-instrumentation-asgi==0.51b0->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
            "Requirement already satisfied: monotonic>=1.5 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: distro>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.28.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: python-dotenv>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
            "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (14.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.10.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<=8.5.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kubernetes>=28.1.0->chromadb) (3.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.27.0->chromadb) (1.3.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Collecting faiss-cpu\n",
            "  Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from faiss-cpu) (24.2)\n",
            "Using cached faiss_cpu-1.10.0-cp311-cp311-manylinux_2_28_x86_64.whl (30.7 MB)\n",
            "Installing collected packages: faiss-cpu\n",
            "Successfully installed faiss-cpu-1.10.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -U langchain\n",
        "! pip install -qU langchain[groq]\n",
        "! pip install langchain_community\n",
        "! pip install -qU langchain-mistralai\n",
        "! pip install pypdf\n",
        "! pip install chromadb\n",
        "! pip install faiss-cpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wqA3lG-Z8_JM"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MArLZLn-yVF"
      },
      "source": [
        "### **set envorinment variables**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRTx1qwL9cDL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ['LANGSMITH_TRACING'] = \"true\"   # for tracing api calls\n",
        "os.environ['LANGSMITH_ENDPOINT'] = \"https://api.smith.langchain.com\"  # provide the url where it will trace\n",
        "os.environ['LANGSMITH_API_KEY'] = \"ENTER_YOUR_LANGCHAIN_API_KEY\" # api key to access langchain\n",
        "os.environ['LANGSMITH_PROJECT'] =\"PROJECT_NAME\" # project name on langsmith\n",
        "os.environ['GROQ_API_KEY'] = \"ENTER_YOUR_GROQ_API_KEY\" # LLM api key for\n",
        "os.environ['MISTRAL_API_KEY'] = \"ENTER_YOUR_MISTRAL_API_KEY\"\n",
        "os.environ['HF_TOKEN'] = 'ENTER_YOUR_HUGGINGFACE_TOKEN'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mGdDYbs-3pZ"
      },
      "source": [
        "### **Load LLaMA3 chat model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eSADChpz-1Wf",
        "outputId": "a4c2f7e8-881b-4b7e-9334-939f872e1ae2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatMistralAI(client=<httpx.Client object at 0x7d34a8e89310>, async_client=<httpx.AsyncClient object at 0x7d34a8f70fd0>, mistral_api_key=SecretStr('**********'), endpoint='https://api.mistral.ai/v1', model='mistral-large-latest')"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "llm = ChatMistralAI(model='mistral-large-latest')\n",
        "llm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vZx5ffK_KK3"
      },
      "source": [
        "### **load the data from Text file**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "lA4rKB9nDXmx"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import TextLoader, WebBaseLoader, PyPDFLoader\n",
        "import bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcbqXLbd_JPa",
        "outputId": "b6d40d71-7d51-4154-f223-ec093e72c5bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'business_data.txt'}, page_content='Acme Corporation\\n1234 Business Park Drive\\nInnovation City, CA 90210\\nPhone: (555) 123-4567\\nEmail: support@acmecorp.com\\nWebsite: www.acmecorp.com\\n\\n--------------------------------------------------------------------------------\\nCompany Overview\\n--------------------------------------------------------------------------------\\nAcme Corporation is a leading provider of innovative business solutions, specializing in state-of-the-art technology and customer support services. Founded in 2005, Acme Corporation has grown into a multinational organization with operations in over 20 countries. Our mission is to empower businesses with tools that drive efficiency, streamline operations, and enhance customer satisfaction.\\n\\nOur core business areas include:\\n- Enterprise Software Solutions\\n- Cloud-Based Customer Support Platforms\\n- Data Analytics and Business Intelligence Tools\\n- Digital Transformation Consulting\\n\\nAt Acme Corporation, we pride ourselves on our commitment to quality and innovation. We invest heavily in research and development, ensuring that our products remain at the forefront of industry standards. Our team of experts works tirelessly to improve our offerings and provide exceptional service to our clients.\\n\\n--------------------------------------------------------------------------------\\nProduct Portfolio\\n--------------------------------------------------------------------------------\\n1. **AcmeSupport Pro**  \\n   A robust customer support platform that integrates AI-driven chatbots and human oversight to deliver seamless service. Key features include:\\n   - Real-time query resolution using advanced language models.\\n   - Multi-channel support (chat, email, phone).\\n   - Comprehensive analytics dashboard for performance tracking.\\n   - Easy integration with CRM systems.\\n\\n2. **AcmeAnalytics 360**  \\n   An all-in-one business intelligence tool designed to provide actionable insights from raw data. This product offers:\\n   - Interactive data visualizations and reporting tools.\\n   - Automated data cleansing and integration.\\n   - Customizable KPIs and performance metrics.\\n   - Integration with leading cloud storage and data warehousing solutions.\\n\\n3. **AcmeCloud Connect**  \\n   A secure cloud services platform that provides scalable IT infrastructure for businesses of all sizes. Services include:\\n   - Virtual servers and managed hosting.\\n   - Data backup and disaster recovery solutions.\\n   - Cloud-based collaboration tools.\\n   - 24/7 technical support with a dedicated account manager.\\n\\n--------------------------------------------------------------------------------\\nCustomer Support and FAQs\\n--------------------------------------------------------------------------------\\nOur customer support team is dedicated to ensuring that every client receives the assistance they need. Below are some frequently asked questions along with our detailed responses:\\n\\n**Q1: How can I access customer support?**  \\nA1: Customers can reach our support team via phone, email, or through our online chat system on the Acme Corporation website. For immediate assistance, we recommend using the chat option, which is powered by our AI-enhanced support platform.\\n\\n**Q2: What is the typical response time for support queries?**  \\nA2: Our average response time is under 5 minutes for chat-based queries and under 30 minutes for emails during business hours. In urgent cases, customers are advised to call our dedicated support hotline.\\n\\n**Q3: How do I report a technical issue?**  \\nA3: To report technical issues, please visit our support portal at www.acmecorp.com/support and fill out the ticket form. Our technical team will review your submission and get back to you as soon as possible.\\n\\n**Q4: Are there any self-help resources available?**  \\nA4: Yes, Acme Corporation offers an extensive knowledge base that includes user guides, video tutorials, and troubleshooting articles. These resources are accessible through our website under the “Resources” section.\\n\\n--------------------------------------------------------------------------------\\nBusiness Policies and Service Level Agreements (SLAs)\\n--------------------------------------------------------------------------------\\nAcme Corporation is committed to providing the highest level of service to our clients. Our SLAs are designed to ensure reliability, quality, and transparency in all our services. Key elements of our policies include:\\n\\n- **Service Uptime:** We guarantee a 99.9% uptime for all our cloud-based services. In case of downtime, customers will be eligible for service credits as per the SLA guidelines.\\n- **Data Security:** All customer data is encrypted both in transit and at rest. We adhere to industry-standard security practices and comply with international data protection regulations.\\n- **Response Time:** Our support team is available 24/7, with specific response times outlined in our SLAs depending on the severity of the issue.\\n- **Maintenance Windows:** Scheduled maintenance is communicated to customers at least 48 hours in advance to minimize disruption.\\n\\n--------------------------------------------------------------------------------\\nAdditional Business Information\\n--------------------------------------------------------------------------------\\nAcme Corporation continually seeks to innovate and improve its product offerings. We actively solicit feedback from our customers and regularly update our products based on industry trends and user needs. Our commitment to excellence is reflected in our robust quality assurance processes and customer-centric approach.\\n\\nFor more detailed information on our products, services, and policies, please refer to our full business documentation available on our intranet or contact your dedicated account manager.\\n\\nThank you for choosing Acme Corporation as your trusted business solutions provider.\\n\\n--------------------------------------------------------------------------------\\n')]\n"
          ]
        }
      ],
      "source": [
        "# loading synthatic data using TextLoader\n",
        "loader = TextLoader('business_data.txt')\n",
        "text_docs = loader.load()\n",
        "print(text_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uuNu_Ou_ulp",
        "outputId": "b03f1896-c956-4734-fc21-75431247d76a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Acme Corporation\n",
            "1234 Business Park Drive\n",
            "Innovation City, CA 90210\n",
            "Phone: (555) 123-4567\n",
            "Email: support@acmecorp.com\n",
            "Website: www.acmecorp.com\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "Company Overview\n",
            "--------------------------------------------------------------------------------\n",
            "Acme Corporation is a leading provider of innovative business solutions, specializing in state-of-the-art technology and customer support services. Founded in 2005, Acme Corporation has grown into a multinational organization with operations in over 20 countries. Our mission \n"
          ]
        }
      ],
      "source": [
        "# print the content inside the document\n",
        "print(text_docs[0].page_content[:600])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jctIpR5wBuBx",
        "outputId": "d1018c9c-1255-478b-957f-6573160ab72d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'business_data.txt'}\n"
          ]
        }
      ],
      "source": [
        "# print the metadata of the loaded docs\n",
        "print(text_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ok74noQzDO5r"
      },
      "source": [
        "### **load the data from Web Url**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9Nx0pesCkBC",
        "outputId": "98275b47-8d89-4e1b-81e1-3c18c3d49a76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Document(metadata={'source': 'https://pytorch.org/tutorials/beginner/basics/intro.html'}, page_content='\\nLearn the Basics¶Created On: Feb 09, 2021 | Last Updated: Nov 04, 2024 | Last Verified: Nov 05, 2024\\nAuthors:\\nSuraj Subramanian,\\nSeth Juarez,\\nCassie Breviu,\\nDmitry Soshnikov,\\nAri Bornstein\\nMost machine learning workflows involve working with data, creating models, optimizing model\\nparameters, and saving the trained models. This tutorial introduces you to a complete ML workflow\\nimplemented in PyTorch, with links to learn more about each of these concepts.\\nWe’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs\\nto one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\\nBag, or Ankle boot.\\nThis tutorial assumes a basic familiarity with Python and Deep Learning concepts.\\n\\nRunning the Tutorial Code¶\\nYou can run this tutorial in a couple of ways:\\n\\nIn the cloud: This is the easiest way to get started! Each section has a “Run in Microsoft Learn” and “Run in Google Colab” link at the top, which opens an integrated notebook in Microsoft Learn or Google Colab, respectively, with the code in a fully-hosted environment.\\nLocally: This option requires you to setup PyTorch and TorchVision first on your local machine (installation instructions). Download the notebook or copy the code into your favorite IDE.\\n\\n\\n\\nHow to Use this Guide¶\\nIf you’re familiar with other deep learning frameworks, check out the 0. Quickstart first\\nto quickly familiarize yourself with PyTorch’s API.\\nIf you’re new to deep learning frameworks, head right into the first section of our step-by-step guide: 1. Tensors.\\n\\n0. Quickstart\\n1. Tensors\\n2. Datasets and DataLoaders\\n3. Transforms\\n4. Build Model\\n5. Automatic Differentiation\\n6. Optimization Loop\\n7. Save, Load and Use Model\\n\\n\\n\\nTotal running time of the script: ( 0 minutes  0.000 seconds)\\n\\n\\nDownload Python source code: intro.py\\n\\n\\nDownload Jupyter notebook: intro.ipynb\\n\\n\\nGallery generated by Sphinx-Gallery\\n\\n')]\n"
          ]
        }
      ],
      "source": [
        "# loading pytorch tutorial content here\n",
        "web_loader = WebBaseLoader(web_path='https://pytorch.org/tutorials/beginner/basics/intro.html', bs_kwargs=dict(parse_only=bs4.SoupStrainer(class_=('section'))))\n",
        "web_docs = web_loader.load()\n",
        "print(web_docs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8B7GgQNJE-2u",
        "outputId": "992c0bdb-7dd2-4f72-a83f-98cefe1dcfea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ne learning workflows involve working with data, creating models, optimizing model\n",
            "parameters, and saving the trained models. This tutorial introduces you to a complete ML workflow\n",
            "implemented in PyTorch, with links to learn more about each of these concepts.\n",
            "We’ll use the FashionMNIST dataset to train a neural network that predicts if an input image belongs\n",
            "to one of the following classes: T-shirt/top, Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker,\n",
            "Bag, or Ankle boot.\n",
            "This tutorial assumes a basic familiarity with Python and Deep Learning \n"
          ]
        }
      ],
      "source": [
        "# print the content inside the web document\n",
        "print(web_docs[0].page_content[200:755])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iYOLAahXFWI4",
        "outputId": "582119b8-a068-4efa-92ad-9f0f88da9c11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'source': 'https://pytorch.org/tutorials/beginner/basics/intro.html'}\n"
          ]
        }
      ],
      "source": [
        "# print the metadata of the loaded docs\n",
        "print(web_docs[0].metadata)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e20N9mVdHXTO"
      },
      "source": [
        "### **load the data from Pdf Document**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "y58kQFU7F68o",
        "outputId": "67011c29-dec2-4022-8b18-81a309b73178"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without super-\\nvised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoning behaviors. However, it encounters challenges such as poor readability, and language\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-\\nR1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the\\nresearch community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models\\n(1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.\\nAIME 2024\\n(Pass@1)\\nCodeforces\\n(Percentile)\\nGPQA Diamond\\n(Pass@1)\\nMATH-500\\n(Pass@1)\\nMMLU\\n(Pass@1)\\nSWE-bench Verified\\n(Resolved)\\n0\\n20\\n40\\n60\\n80\\n100Accuracy / Percentile (%)\\n79.8\\n96.3\\n71.5\\n97.3\\n90.8\\n49.2\\n79.2\\n96.6\\n75.7\\n96.4\\n91.8\\n48.9\\n72.6\\n90.6\\n62.1\\n94.3\\n87.4\\n36.8\\n63.6\\n93.4\\n60.0\\n90.0\\n85.2\\n41.6\\n39.2\\n58.7 59.1\\n90.2\\n88.5\\n42.0\\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\\nFigure 1 |Benchmark performance of DeepSeek-R1.\\narXiv:2501.12948v1  [cs.CL]  22 Jan 2025'"
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pdf_loader = PyPDFLoader('/content/deepseek_r1_paper.pdf')\n",
        "pdf_data = pdf_loader.load()\n",
        "# print(pdf_data[:500])\n",
        "pdf_data[0].page_content[:5000]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31-Luh_EIRUi",
        "outputId": "3942263a-8850-4e9f-ec4d-a0ebd9af37b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/deepseek_r1_paper.pdf', 'total_pages': 22, 'page': 0, 'page_label': '1'}\n"
          ]
        }
      ],
      "source": [
        "# print the metadata of the loaded docs\n",
        "print(pdf_data[0].metadata)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H35ND7bfICdu",
        "outputId": "ccc1f4ca-be0b-4c04-e416-566e485ea582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\n",
            "reasoning behaviors. However, it encounters challenges such as poor readability, and language\n",
            "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
            "DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL.\n"
          ]
        }
      ],
      "source": [
        "# print the content inside the pdf document\n",
        "print(pdf_data[0].page_content[400:750])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prtD0klIh9qQ"
      },
      "source": [
        "### **Split the Documents into the Chunks**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "hysN8R16iEFo"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "pdf_docs = splitter.split_documents(pdf_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VYSVlOui1jd",
        "outputId": "31692168-0f81-4131-89c8-114d1fe03d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "total documents after splitting text into chunks : 70\n"
          ]
        }
      ],
      "source": [
        "# print the lenght of pdf docs\n",
        "print('total documents after splitting text into chunks :', len(pdf_docs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmVbZIjFi7Ol",
        "outputId": "21731eb2-052e-413c-efd6-cefa611f0a6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "page_content='and 57.2% on LiveCodeBench. These results significantly outperform previous open-\n",
            "source models and are comparable to o1-mini. We open-source distilled 1.5B, 7B, 8B, 14B,\n",
            "32B, and 70B checkpoints based on Qwen2.5 and Llama3 series to the community.\n",
            "1.2. Summary of Evaluation Results\n",
            "• Reasoning tasks: (1) DeepSeek-R1 achieves a score of 79.8% Pass@1 on AIME 2024, slightly\n",
            "surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\n",
            "performing on par with OpenAI-o1-1217 and significantly outperforming other models. (2)\n",
            "On coding-related tasks, DeepSeek-R1 demonstrates expert level in code competition tasks,\n",
            "as it achieves 2,029 Elo rating on Codeforces outperforming 96.3% human participants in\n",
            "the competition. For engineering-related tasks, DeepSeek-R1 performs slightly better than\n",
            "DeepSeek-V3, which could help developers in real world tasks.\n",
            "• Knowledge: On benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond, DeepSeek-' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/deepseek_r1_paper.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}\n",
            "++++++++++++++++++++++++++++++++++++\n",
            "page_content='R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\n",
            "of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\n",
            "performance is slightly below that of OpenAI-o1-1217 on these benchmarks, DeepSeek-R1\n",
            "surpasses other closed-source models, demonstrating its competitive edge in educational\n",
            "tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\n",
            "demonstrating its capability in handling fact-based queries. A similar trend is observed\n",
            "where OpenAI-o1 surpasses 4o on this benchmark.\n",
            "4' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/deepseek_r1_paper.pdf', 'total_pages': 22, 'page': 3, 'page_label': '4'}\n",
            "++++++++++++++++++++++++++++++++++++\n",
            "page_content='• Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\n",
            "general question answering, editing, summarization, and more. It achieves an impressive\n",
            "length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\n",
            "naHard, showcasing its strong ability to intelligently handle non-exam-oriented queries.\n",
            "Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\n",
            "long-context understanding, substantially outperforming DeepSeek-V3 on long-context\n",
            "benchmarks.\n",
            "2. Approach\n",
            "2.1. Overview\n",
            "Previous work has heavily relied on large amounts of supervised data to enhance model\n",
            "performance. In this study, we demonstrate that reasoning capabilities can be significantly\n",
            "improved through large-scale reinforcement learning (RL), even without using supervised\n",
            "fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with' metadata={'producer': 'pdfTeX-1.40.25', 'creator': 'LaTeX with hyperref', 'creationdate': '2025-01-23T01:45:31+00:00', 'author': '', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'subject': '', 'title': '', 'trapped': '/False', 'source': '/content/deepseek_r1_paper.pdf', 'total_pages': 22, 'page': 4, 'page_label': '5'}\n"
          ]
        }
      ],
      "source": [
        "# print first 3 splitted docs\n",
        "print(pdf_docs[10])\n",
        "print('++++++++++++++++++++++++++++++++++++')\n",
        "print(pdf_docs[11])\n",
        "print('++++++++++++++++++++++++++++++++++++')\n",
        "print(pdf_docs[12])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iD1NNywqZxQs"
      },
      "source": [
        "### **create the Vector Embeddings and Store in VectorDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsY6WYEoaM35",
        "outputId": "d75972f2-1b34-437b-8b86-bfe67d344a40"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_mistralai/embeddings.py:181: UserWarning: Could not download mistral tokenizer from Huggingface for calculating batch sizes. Set a Huggingface token via the HF_TOKEN environment variable to download the real tokenizer. Falling back to a dummy tokenizer that uses `len()`.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from langchain_mistralai import MistralAIEmbeddings\n",
        "\n",
        "# using mistral opensource embeddings\n",
        "embedding_model = MistralAIEmbeddings(model='mistral-embed')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zMU9-P15b3yv",
        "outputId": "977ba2a0-2401-4dc2-f393-5a6045618d58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<langchain_community.vectorstores.chroma.Chroma object at 0x7d34a8f1ded0>\n"
          ]
        }
      ],
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "# store first 50 documents into vectorstore\n",
        "db = Chroma.from_documents(pdf_docs[:20], embedding_model)\n",
        "print(db)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ySp9nWLko7B"
      },
      "source": [
        "### **search the similar docs to user Query from VectorDB**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Br8PcuDrjZZr",
        "outputId": "52777578-c43f-4db9-e37f-a261458285dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4\n"
          ]
        }
      ],
      "source": [
        "query = '''Using the reasoning data generated by DeepSeek-R1, we fine-tuned several dense models\n",
        "that are widely used in the research community.'''\n",
        "results = db.similarity_search_with_relevance_scores(query, k=4)\n",
        "print(len(results))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0btxjElk5r5",
        "outputId": "0fa28b9f-a3fb-4848-84af-b40706fdc82d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "similarity score : 0.869409981327587\n",
            "---------------------------------\n",
            "content >>>  such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\n",
            "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\n",
            "into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\n",
            "as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n",
            "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\n",
            "32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\n",
            "RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\n",
            "cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\n",
            "et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\n",
            "QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\n",
            "new record on the reasoning benchmarks among dense models.\n",
            "3\n",
            "\n",
            "---------------------------------\n",
            "\n",
            "similarity score : 0.869409981327587\n",
            "---------------------------------\n",
            "content >>>  such as writing, factual QA, and self-cognition, and then retrain the DeepSeek-V3-Base model.\n",
            "After fine-tuning with the new data, the checkpoint undergoes an additional RL process, taking\n",
            "into account prompts from all scenarios. After these steps, we obtained a checkpoint referred to\n",
            "as DeepSeek-R1, which achieves performance on par with OpenAI-o1-1217.\n",
            "We further explore distillation from DeepSeek-R1 to smaller dense models. Using Qwen2.5-\n",
            "32B (Qwen, 2024b) as the base model, direct distillation from DeepSeek-R1 outperforms applying\n",
            "RL on it. This demonstrates that the reasoning patterns discovered by larger base models are cru-\n",
            "cial for improving reasoning capabilities. We open-source the distilled Qwen and Llama (Dubey\n",
            "et al., 2024) series. Notably, our distilled 14B model outperforms state-of-the-art open-source\n",
            "QwQ-32B-Preview (Qwen, 2024a) by a large margin, and the distilled 32B and 70B models set a\n",
            "new record on the reasoning benchmarks among dense models.\n",
            "3\n",
            "\n",
            "---------------------------------\n",
            "\n",
            "similarity score : 0.857509228594657\n",
            "---------------------------------\n",
            "content >>>  reasoning behaviors. After thousands of RL steps, DeepSeek-R1-Zero exhibits super performance\n",
            "on reasoning benchmarks. For instance, the pass@1 score on AIME 2024 increases from 15.6% to\n",
            "71.0%, and with majority voting, the score further improves to 86.7%, matching the performance\n",
            "of OpenAI-o1-0912.\n",
            "However, DeepSeek-R1-Zero encounters challenges such as poor readability, and language\n",
            "mixing. To address these issues and further enhance reasoning performance, we introduce\n",
            "DeepSeek-R1, which incorporates a small amount of cold-start data and a multi-stage training\n",
            "pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\n",
            "DeepSeek-V3-Base model. Following this, we perform reasoning-oriented RL like DeepSeek-R1-\n",
            "Zero. Upon nearing convergence in the RL process, we create new SFT data through rejection\n",
            "sampling on the RL checkpoint, combined with supervised data from DeepSeek-V3 in domains\n",
            "\n",
            "---------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('---------------------------------')\n",
        "print('similarity score :', results[0][1])\n",
        "print('---------------------------------')\n",
        "print('content >>> ', results[0][0].page_content)\n",
        "print()\n",
        "print('---------------------------------')\n",
        "print()\n",
        "print('similarity score :', results[1][1])\n",
        "print('---------------------------------')\n",
        "print('content >>> ', results[1][0].page_content)\n",
        "print()\n",
        "print('---------------------------------')\n",
        "print()\n",
        "print('similarity score :', results[3][1])\n",
        "print('---------------------------------')\n",
        "print('content >>> ', results[3][0].page_content)\n",
        "print()\n",
        "print('---------------------------------')\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DUehSivyyZZr"
      },
      "source": [
        "### **create prompt template for LLM calling**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "tjhyu8SYpUfn"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "        Answer the following question based on the provide context.\n",
        "        Thnink step by step before providing the answer.\n",
        "\n",
        "        <context>\n",
        "        {context}\n",
        "        </context>\n",
        "\n",
        "        Question: {input}\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDyXZpIP1Af5"
      },
      "source": [
        "### **create Document Chains**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q06uz42YzFpL",
        "outputId": "0e12bf5d-d78f-49ec-d65d-f37110c1f369"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  context: RunnableLambda(format_docs)\n",
              "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
              "| ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n        Answer the following question based on the provide context.\\n        Thnink step by step before providing the answer.\\n\\n        <context>\\n        {context}\\n        </context>\\n\\n        Question: {input}'), additional_kwargs={})])\n",
              "| ChatMistralAI(client=<httpx.Client object at 0x7d34a8e89310>, async_client=<httpx.AsyncClient object at 0x7d34a8f70fd0>, mistral_api_key=SecretStr('**********'), endpoint='https://api.mistral.ai/v1', model='mistral-large-latest')\n",
              "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "\n",
        "# this will create the chain to pass list of documents to llm with provifde promopt format\n",
        "# it must contins \"context\" variable inside the promot\n",
        "document_chain = create_stuff_documents_chain(llm, prompt)\n",
        "document_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0xECvwgG2eKt"
      },
      "source": [
        "### **create Retriever**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8bhsCz02U0u",
        "outputId": "e69183ff-be5c-4ebd-d91e-e4fef67b3415"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "VectorStoreRetriever(tags=['Chroma', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d34a8f1ded0>, search_kwargs={})"
            ]
          },
          "execution_count": 84,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# create retriever directly from verctorstore\n",
        "retriever = db.as_retriever()\n",
        "retriever"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87Ha8eFW3CqP"
      },
      "source": [
        "### **create Retriever Chain**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1X5vRzw02wMu",
        "outputId": "0a16233e-71ba-4323-bc92-521c4839e823"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
              "           | VectorStoreRetriever(tags=['Chroma', 'MistralAIEmbeddings'], vectorstore=<langchain_community.vectorstores.chroma.Chroma object at 0x7d34a8f1ded0>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
              "})\n",
              "| RunnableAssign(mapper={\n",
              "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "              context: RunnableLambda(format_docs)\n",
              "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
              "            | ChatPromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'input'], input_types={}, partial_variables={}, template='\\n        Answer the following question based on the provide context.\\n        Thnink step by step before providing the answer.\\n\\n        <context>\\n        {context}\\n        </context>\\n\\n        Question: {input}'), additional_kwargs={})])\n",
              "            | ChatMistralAI(client=<httpx.Client object at 0x7d34a8e89310>, async_client=<httpx.AsyncClient object at 0x7d34a8f70fd0>, mistral_api_key=SecretStr('**********'), endpoint='https://api.mistral.ai/v1', model='mistral-large-latest')\n",
              "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
              "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from langchain.chains import create_retrieval_chain\n",
        "\n",
        "# create_retriever_chain takes retriever and document chain as input to\n",
        "retriever_chain = create_retrieval_chain(retriever, document_chain)\n",
        "retriever_chain"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsLPNyEm56z8"
      },
      "source": [
        "### **Ask you Query to RAG System (LLM + Context from VectorDB)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnabVGLB5DSp",
        "outputId": "1f51db7b-d1b7-43ce-ac85-e6c88d9f66dd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "dict_keys(['input', 'context', 'answer'])"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = retriever_chain.invoke({'input': 'what is Group Relative Policy Optimization ?'})\n",
        "response.keys()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Nk1gRVR7KRq",
        "outputId": "e99b3cf8-5bea-4703-ab7d-f34d3e0c380d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Document(metadata={'author': '', 'creationdate': '2025-01-23T01:45:31+00:00', 'creator': 'LaTeX with hyperref', 'keywords': '', 'moddate': '2025-01-23T01:45:31+00:00', 'page': 4, 'page_label': '5', 'producer': 'pdfTeX-1.40.25', 'ptex.fullbanner': 'This is pdfTeX, Version 3.141592653-2.6-1.40.25 (TeX Live 2023) kpathsea version 6.3.5', 'source': '/content/deepseek_r1_paper.pdf', 'subject': '', 'title': '', 'total_pages': 22, 'trapped': '/False'}, page_content='brief overview of our RL algorithm, followed by the presentation of some exciting results, and\\nhope this provides the community with valuable insights.\\n2.2.1. Reinforcement Learning Algorithm\\nGroup Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\\nRelative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\\ntypically the same size as the policy model, and estimates the baseline from group scores instead.\\nSpecifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, ··· , 𝑜𝐺}from the old\\npolicy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:\\nJ𝐺𝑅𝑃𝑂 (𝜃)= E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\\n𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑 (𝑂|𝑞)]\\n1\\n𝐺\\n𝐺∑︁\\n𝑖=1\\n\\x12\\nmin\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞)𝐴𝑖, clip\\n\\x12 𝜋𝜃(𝑜𝑖|𝑞)\\n𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞), 1−𝜀, 1+𝜀\\n\\x13\\n𝐴𝑖\\n\\x13\\n−𝛽D𝐾𝐿\\n\\x00\\n𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01\\x13\\n, (1)\\nD𝐾𝐿\\n\\x00\\n𝜋𝜃||𝜋𝑟𝑒𝑓\\n\\x01 =\\n𝜋𝑟𝑒𝑓 (𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −log\\n𝜋𝑟𝑒𝑓 (𝑜𝑖|𝑞)\\n𝜋𝜃(𝑜𝑖|𝑞) −1, (2)\\nwhere 𝜀 and 𝛽 are hyper-parameters, and 𝐴𝑖 is the advantage, computed using a group of')"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# print the context docs\n",
        "response['context'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bpSQcFjd8QoF",
        "outputId": "de5ee1a7-1471-45b2-b73e-38ad400ae7bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "brief overview of our RL algorithm, followed by the presentation of some exciting results, and\n",
            "hope this provides the community with valuable insights.\n",
            "2.2.1. Reinforcement Learning Algorithm\n",
            "Group Relative Policy OptimizationIn order to save the training costs of RL, we adopt Group\n",
            "Relative Policy Optimization (GRPO) (Shao et al., 2024), which foregoes the critic model that is\n",
            "typically the same size as the policy model, and estimates the baseline from group scores instead.\n",
            "Specifically, for each question 𝑞, GRPO samples a group of outputs {𝑜1, 𝑜2, ··· , 𝑜𝐺}from the old\n",
            "policy 𝜋𝜃𝑜𝑙𝑑 and then optimizes the policy model 𝜋𝜃 by maximizing the following objective:\n",
            "J𝐺𝑅𝑃𝑂 (𝜃)= E[𝑞∼𝑃(𝑄), {𝑜𝑖}𝐺\n",
            "𝑖=1 ∼𝜋𝜃𝑜𝑙𝑑 (𝑂|𝑞)]\n",
            "1\n",
            "𝐺\n",
            "𝐺∑︁\n",
            "𝑖=1\n",
            "\u0012\n",
            "min\n",
            "\u0012 𝜋𝜃(𝑜𝑖|𝑞)\n",
            "𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞)𝐴𝑖, clip\n",
            "\u0012 𝜋𝜃(𝑜𝑖|𝑞)\n",
            "𝜋𝜃𝑜𝑙𝑑 (𝑜𝑖|𝑞), 1−𝜀, 1+𝜀\n",
            "\u0013\n",
            "𝐴𝑖\n",
            "\u0013\n",
            "−𝛽D𝐾𝐿\n",
            "\u0000\n",
            "𝜋𝜃||𝜋𝑟𝑒𝑓\n",
            "\u0001\u0013\n",
            ", (1)\n",
            "D𝐾𝐿\n",
            "\u0000\n",
            "𝜋𝜃||𝜋𝑟𝑒𝑓\n",
            "\u0001 =\n",
            "𝜋𝑟𝑒𝑓 (𝑜𝑖|𝑞)\n",
            "𝜋𝜃(𝑜𝑖|𝑞) −log\n",
            "𝜋𝑟𝑒𝑓 (𝑜𝑖|𝑞)\n",
            "𝜋𝜃(𝑜𝑖|𝑞) −1, (2)\n",
            "where 𝜀 and 𝛽 are hyper-parameters, and 𝐴𝑖 is the advantage, computed using a group of\n"
          ]
        }
      ],
      "source": [
        "# print the first 2 context docs\n",
        "print(response['context'][0].page_content)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Yxi11FG9ssy",
        "outputId": "c55b31eb-0516-459b-d9d4-956b6b5bc350"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To understand what Group Relative Policy Optimization (GRPO) is, let's break down the provided context step by step:\n",
            "\n",
            "1. **Purpose**: GRPO is an approach adopted to save the training costs of reinforcement learning (RL) by foregoing the critic model, which is typically the same size as the policy model. Instead, it estimates the baseline from group scores.\n",
            "\n",
            "2. **Mechanism**:\n",
            "   - For each question \\( q \\), GRPO samples a group of outputs \\(\\{o_1, o_2, \\ldots, o_G\\}\\) from the old policy \\(\\pi_{\\theta_{old}}\\).\n",
            "   - It then optimizes the policy model \\(\\pi_{\\theta}\\) by maximizing the objective function \\(J_{GRPO}(\\theta)\\).\n",
            "\n",
            "3. **Objective Function**:\n",
            "   - The objective function is given by:\n",
            "     \\[\n",
            "     J_{GRPO}(\\theta) = E\\left[q \\sim P(Q), \\{o_i\\}_{i=1}^G \\sim \\pi_{\\theta_{old}}(O|q)\\right] \\frac{1}{G} \\sum_{i=1}^G \\left[ \\min \\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}A_i, \\text{clip}\\left( \\frac{\\pi_{\\theta}(o_i|q)}{\\pi_{\\theta_{old}}(o_i|q)}, 1-\\epsilon, 1+\\epsilon \\right)A_i \\right) - \\beta D_{KL} \\left( \\pi_{\\theta} || \\pi_{ref} \\right) \\right]\n",
            "     \\]\n",
            "   - Where \\(\\epsilon\\) and \\(\\beta\\) are hyper-parameters, and \\(A_i\\) is the advantage, computed using a group of rewards corresponding to the outputs within each group.\n",
            "\n",
            "4. **Advantage Calculation**:\n",
            "   - The advantage \\(A_i\\) is computed as:\n",
            "     \\[\n",
            "     A_i = r_i - \\text{mean}(\\{r_1, r_2, \\ldots, r_G\\}) / \\text{std}(\\{r_1, r_2, \\ldots, r_G\\})\n",
            "     \\]\n",
            "   - Here, \\(\\{r_1, r_2, \\ldots, r_G\\}\\) are the rewards corresponding to the outputs within each group.\n",
            "\n",
            "In summary, **Group Relative Policy Optimization (GRPO)** is a reinforcement learning algorithm designed to reduce training costs by eliminating the need for a critic model. It optimizes the policy model by sampling outputs from the old policy and using group-based rewards to estimate the baseline and compute advantages. This approach helps in improving the policy model while keeping the training efficient.\n"
          ]
        }
      ],
      "source": [
        "# print the answer\n",
        "print(response['answer'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxdez1Uh9Q1I"
      },
      "source": [
        "### **Ask Another Query**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "SbXur9UQ85HW"
      },
      "outputs": [],
      "source": [
        "response = retriever_chain.invoke({'input': '''One of the most remarkable aspects of this self-evolution is the emergence of sophisticated\n",
        "behaviors as the test-time computation increases. why is that ?'''})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROycRTHO9_rL",
        "outputId": "bf8e6108-5867-4d4b-daf9-381ee54febf5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Introduction\n",
            "In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\n",
            "evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\n",
            "towards Artificial General Intelligence (AGI).\n",
            "Recently, post-training has emerged as an important component of the full training pipeline.\n",
            "It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\n",
            "to user preferences, all while requiring relatively minimal computational resources against\n",
            "pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\n",
            "were the first to introduce inference-time scaling by increasing the length of the Chain-of-\n",
            "Thought reasoning process. This approach has achieved significant improvements in various\n",
            "reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\n",
            "of effective test-time scaling remains an open question for the research community. Several prior\n",
            "--------------------------------\n",
            "1. Introduction\n",
            "In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\n",
            "evolution (Anthropic, 2024; Google, 2024; OpenAI, 2024a), progressively diminishing the gap\n",
            "towards Artificial General Intelligence (AGI).\n",
            "Recently, post-training has emerged as an important component of the full training pipeline.\n",
            "It has been shown to enhance accuracy on reasoning tasks, align with social values, and adapt\n",
            "to user preferences, all while requiring relatively minimal computational resources against\n",
            "pre-training. In the context of reasoning capabilities, OpenAI’s o1 (OpenAI, 2024b) series models\n",
            "were the first to introduce inference-time scaling by increasing the length of the Chain-of-\n",
            "Thought reasoning process. This approach has achieved significant improvements in various\n",
            "reasoning tasks, such as mathematics, coding, and scientific reasoning. However, the challenge\n",
            "of effective test-time scaling remains an open question for the research community. Several prior\n",
            "--------------------------------\n"
          ]
        }
      ],
      "source": [
        "# print the first 2 context docs\n",
        "print(response['context'][0].page_content)\n",
        "print('--------------------------------')\n",
        "print(response['context'][1].page_content)\n",
        "print('--------------------------------')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pSOuR4jz91-l",
        "outputId": "f49738bf-9f97-4f16-876d-770d4b0a126b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To answer why the emergence of sophisticated behaviors is one of the most remarkable aspects of the self-evolution as test-time computation increases, we need to consider several points from the provided context:\n",
            "\n",
            "1. **Post-Training Enhancements**: The context mentions that post-training has become crucial in enhancing the accuracy of reasoning tasks, aligning with social values, and adapting to user preferences. This suggests that post-training methods, including reinforcement learning (RL), are effective in improving model performance.\n",
            "\n",
            "2. **Inference-Time Scaling**: OpenAI’s o1 series models introduced inference-time scaling by extending the Chain-of-Thought reasoning process. This approach has shown significant improvements in various reasoning tasks. The ability to scale reasoning during inference suggests that models can become more sophisticated as they are allowed more computational resources to think through problems.\n",
            "\n",
            "3. **Reinforcement Learning (RL) Process**: The model DeepSeek-R1-Zero demonstrates a steady and consistent enhancement in performance as RL training advances. The average pass@1 score on the AIME 2024 benchmark shows a notable increase from 15.6% to 71.0%. This indicates that the RL process is effectively improving the model's capabilities over time.\n",
            "\n",
            "4. **Natural Progression**: The context emphasizes observing the model's natural progression during the RL process without content-specific biases. This allows for an unbiased view of how the model evolves its reasoning capabilities.\n",
            "\n",
            "Given these points, the emergence of sophisticated behaviors as test-time computation increases is remarkable because it shows that the model can dynamically improve its reasoning and problem-solving abilities without extensive pre-training. This self-evolution process, facilitated by RL and inference-time scaling, allows the model to achieve complex reasoning tasks more effectively, demonstrating the potential for significant advancements in AI capabilities.\n",
            "\n",
            "In summary, the sophisticated behaviors emerge due to the model's ability to leverage additional computational resources during inference, allowing it to refine and enhance its reasoning processes over time.\n"
          ]
        }
      ],
      "source": [
        "# print the answer\n",
        "print(response['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "3OwTo9nx-gL3"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
